{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9zHO9fSimCyN"
      },
      "outputs": [],
      "source": [
        "## Combined data then splitted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import os\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import regularizers, optimizers\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint , LearningRateScheduler\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,roc_curve, auc,confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "8Ag_440zm22P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DfsLwiI2m5M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d24272d-c195-4477-f2fc-20214b864a43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/all_data.zip"
      ],
      "metadata": {
        "id": "OnG_MGF_pdcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/all_data_class_combined.zip"
      ],
      "metadata": {
        "id": "DcYOTLpnpzLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path_all= '/content/all_data'\n",
        "\n",
        "class_names = sorted(os.listdir(img_path_all))\n",
        "class_names"
      ],
      "metadata": {
        "id": "b-WsRU5wm7p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4561f62-767c-4aee-851c-3092f55020e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " 'MildDemented',\n",
              " 'ModerateDemented',\n",
              " 'NonDemented',\n",
              " 'VeryMildDemented']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.DataFrame(columns=(\"image\",\"label\"))\n",
        "for i in class_names:\n",
        "    if i != \".DS_Store\":\n",
        "        for j in os.listdir(img_path_all+ \"/\" + i):\n",
        "            df.loc[len(df.index)] = [j,i]\n",
        "df.to_csv(\"csv_file_all.csv\")"
      ],
      "metadata": {
        "id": "kifqUjhWnjbi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Eb06WfosHjeo",
        "outputId": "a751b22d-8d5b-4690-8898-d8de7a1407dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            image         label\n",
              "0  mildDem151.jpg  MildDemented\n",
              "1  mildDem694.jpg  MildDemented\n",
              "2     32 (19).jpg  MildDemented\n",
              "3  mildDem180.jpg  MildDemented\n",
              "4  mildDem614.jpg  MildDemented"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2790c3a-020d-4273-a21f-afe2d1b737c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>mildDem151.jpg</td>\n",
              "      <td>MildDemented</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mildDem694.jpg</td>\n",
              "      <td>MildDemented</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32 (19).jpg</td>\n",
              "      <td>MildDemented</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mildDem180.jpg</td>\n",
              "      <td>MildDemented</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mildDem614.jpg</td>\n",
              "      <td>MildDemented</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2790c3a-020d-4273-a21f-afe2d1b737c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2790c3a-020d-4273-a21f-afe2d1b737c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2790c3a-020d-4273-a21f-afe2d1b737c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3fdac004-4579-47f1-b7e4-3d2680365499\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3fdac004-4579-47f1-b7e4-3d2680365499')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3fdac004-4579-47f1-b7e4-3d2680365499 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV files\n",
        "df = pd.read_csv(\"csv_file_all.csv\")\n",
        "\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "PYlEwAY6nl9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4d45eb-8e1e-4f2f-db20-c3b5970ea536"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NonDemented         3200\n",
            "VeryMildDemented    2240\n",
            "MildDemented         896\n",
            "ModerateDemented      64\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aKNN4uEsn0bS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df), len(test_df) , len(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HSGAVAnn3kR",
        "outputId": "f6ecb17f-073c-48df-accd-c1eeb4242850"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4096, 1280, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = IDG(rescale=1/255, horizontal_flip=True, rotation_range=20)\n",
        "\n",
        "train_images = train_gen.flow_from_dataframe(dataframe=train_df, directory='/content/all_data_class_combined/', x_col=\"image\", y_col=\"label\", class_mode=\"categorical\", shuffle=True, batch_size=16, color_mode=\"grayscale\", target_size=(176,176))\n",
        "validation_images = train_gen.flow_from_dataframe(dataframe=val_df, directory='/content/all_data_class_combined/', x_col=\"image\", y_col=\"label\", class_mode=\"categorical\", shuffle=True, batch_size=16, color_mode=\"grayscale\", target_size=(176,176))\n",
        "test_images = train_gen.flow_from_dataframe(dataframe=test_df, directory='/content/all_data_class_combined/', x_col=\"image\", y_col=\"label\", class_mode=\"categorical\", shuffle=True, batch_size=16, color_mode=\"grayscale\", target_size=(176,176))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQoviA4qn5ze",
        "outputId": "6564f950-a3e3-4c47-9b7a-307506f00fc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4096 validated image filenames belonging to 4 classes.\n",
            "Found 1024 validated image filenames belonging to 4 classes.\n",
            "Found 1280 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = Counter(train_images.classes)\n",
        "counter_val = Counter(validation_images.classes)\n",
        "counter_test = Counter(test_images.classes)\n",
        "print(counter.items())\n",
        "print(counter_val.items())\n",
        "print(counter_test.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D05FoZ_rn-Ew",
        "outputId": "0fe149ee-81ca-4b32-951f-631f1f46d8cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(0, 557), (3, 1456), (2, 2033), (1, 50)])\n",
            "dict_items([(3, 354), (2, 524), (0, 138), (1, 8)])\n",
            "dict_items([(2, 643), (3, 430), (0, 201), (1, 6)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = [557,50,2033 ,1456]\n",
        "\n",
        "class_weights = compute_class_weight(class_weight = 'balanced', classes=np.unique(train_images.classes), y= train_images.classes)\n",
        "\n",
        "class_weights_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "ECM2QycPowpT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px9B2qUQoz-n",
        "outputId": "63a789ef-3828-4736-baad-ab9d3f34c595"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.8384201077199283, 1: 20.48, 2: 0.5036891293654697, 3: 0.7032967032967034}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model1"
      ],
      "metadata": {
        "id": "U8nHjKOicH_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = models.Sequential()\n",
        "model1.add(layers.Conv2D(128, (3, 3), padding='same', input_shape=(176, 176, 1)))\n",
        "model1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(layers.Dropout(0.3))\n",
        "model1.add(layers.Activation(\"relu\"))\n",
        "model1.add(layers.Conv2D(64,(3, 3),  padding='same'))\n",
        "model1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(layers.Dropout(0.3))\n",
        "model1.add(layers.Activation(\"relu\"))\n",
        "model1.add(layers.Conv2D(32,(3, 3),  padding='same'))\n",
        "model1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(layers.Dropout(0.3))\n",
        "model1.add(layers.Activation(\"relu\"))\n",
        "model1.add(layers.Conv2D(16,(3, 3),  padding='same'))\n",
        "model1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model1.add(BatchNormalization())\n",
        "model1.add(layers.Dropout(0.3))\n",
        "model1.add(layers.Activation(\"relu\"))\n",
        "model1.add(layers.Flatten())\n",
        "model1.add(layers.Dense(512))\n",
        "model1.add(layers.Dropout(0.3))\n",
        "model1.add(layers.Dense(4, activation='softmax'))\n",
        "model1.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "JOUW2dNwoAnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afTKWM8NoDvv",
        "outputId": "a020338b-ead4-4bfd-9a08-e83aeca2d0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 176, 176, 128)     1280      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 88, 88, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 88, 88, 128)       512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 88, 88, 128)       0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 88, 88, 128)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 88, 88, 64)        73792     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 44, 44, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 44, 44, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 44, 44, 64)        0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 44, 44, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 44, 44, 32)        18464     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 22, 22, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 22, 22, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 22, 22, 32)        0         \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 22, 22, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 22, 22, 16)        4624      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 11, 11, 16)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 11, 11, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 11, 11, 16)        0         \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 11, 11, 16)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1936)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               991744    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1092916 (4.17 MB)\n",
            "Trainable params: 1092436 (4.17 MB)\n",
            "Non-trainable params: 480 (1.88 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.load_weights('/content/shuffled_model1_1.h5')"
      ],
      "metadata": {
        "id": "MFk-QbpcoGMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint('best_model_1.h5', save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=2)\n",
        "history = model1.fit(train_images, validation_data=validation_images ,callbacks=[model_checkpoint], class_weight=class_weights_dict , epochs= 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqBoGTYvo2un",
        "outputId": "c757a9dd-b350-4234-90dc-656d7cf512d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 4.4604 - accuracy: 0.2930\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00781, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 33s 181ms/step - loss: 4.4604 - accuracy: 0.2930 - val_loss: 1.5304 - val_accuracy: 0.0078\n",
            "Epoch 2/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 3.3104 - accuracy: 0.3069\n",
            "Epoch 2: val_accuracy did not improve from 0.00781\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 3.3104 - accuracy: 0.3069 - val_loss: 1.5465 - val_accuracy: 0.0078\n",
            "Epoch 3/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 2.2096 - accuracy: 0.3215\n",
            "Epoch 3: val_accuracy improved from 0.00781 to 0.13477, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 24s 184ms/step - loss: 2.2096 - accuracy: 0.3215 - val_loss: 1.6099 - val_accuracy: 0.1348\n",
            "Epoch 4/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.7169 - accuracy: 0.3450\n",
            "Epoch 4: val_accuracy did not improve from 0.13477\n",
            "128/128 [==============================] - 25s 198ms/step - loss: 1.7169 - accuracy: 0.3450 - val_loss: 1.5939 - val_accuracy: 0.0371\n",
            "Epoch 5/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.5070 - accuracy: 0.3657\n",
            "Epoch 5: val_accuracy improved from 0.13477 to 0.14941, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 24s 191ms/step - loss: 1.5070 - accuracy: 0.3657 - val_loss: 1.4640 - val_accuracy: 0.1494\n",
            "Epoch 6/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.4192 - accuracy: 0.3831\n",
            "Epoch 6: val_accuracy did not improve from 0.14941\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 1.4192 - accuracy: 0.3831 - val_loss: 1.5716 - val_accuracy: 0.1104\n",
            "Epoch 7/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.3774 - accuracy: 0.3770\n",
            "Epoch 7: val_accuracy improved from 0.14941 to 0.48242, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 1.3774 - accuracy: 0.3770 - val_loss: 1.0703 - val_accuracy: 0.4824\n",
            "Epoch 8/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2860 - accuracy: 0.3921\n",
            "Epoch 8: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 24s 186ms/step - loss: 1.2860 - accuracy: 0.3921 - val_loss: 1.4563 - val_accuracy: 0.2441\n",
            "Epoch 9/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2490 - accuracy: 0.3936\n",
            "Epoch 9: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 25s 199ms/step - loss: 1.2490 - accuracy: 0.3936 - val_loss: 1.4556 - val_accuracy: 0.2412\n",
            "Epoch 10/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2652 - accuracy: 0.3967\n",
            "Epoch 10: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 25s 197ms/step - loss: 1.2652 - accuracy: 0.3967 - val_loss: 1.2331 - val_accuracy: 0.4316\n",
            "Epoch 11/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1924 - accuracy: 0.4014\n",
            "Epoch 11: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 1.1924 - accuracy: 0.4014 - val_loss: 1.3165 - val_accuracy: 0.3896\n",
            "Epoch 12/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1643 - accuracy: 0.4270\n",
            "Epoch 12: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 24s 191ms/step - loss: 1.1643 - accuracy: 0.4270 - val_loss: 1.3293 - val_accuracy: 0.3369\n",
            "Epoch 13/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1695 - accuracy: 0.4351\n",
            "Epoch 13: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 24s 183ms/step - loss: 1.1695 - accuracy: 0.4351 - val_loss: 1.0988 - val_accuracy: 0.4678\n",
            "Epoch 14/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2077 - accuracy: 0.4385\n",
            "Epoch 14: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 24s 185ms/step - loss: 1.2077 - accuracy: 0.4385 - val_loss: 1.2734 - val_accuracy: 0.3955\n",
            "Epoch 15/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1777 - accuracy: 0.4236\n",
            "Epoch 15: val_accuracy did not improve from 0.48242\n",
            "128/128 [==============================] - 25s 198ms/step - loss: 1.1777 - accuracy: 0.4236 - val_loss: 1.2230 - val_accuracy: 0.4287\n",
            "Epoch 16/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1477 - accuracy: 0.4373\n",
            "Epoch 16: val_accuracy improved from 0.48242 to 0.48535, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 25s 198ms/step - loss: 1.1477 - accuracy: 0.4373 - val_loss: 1.0857 - val_accuracy: 0.4854\n",
            "Epoch 17/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1142 - accuracy: 0.4631\n",
            "Epoch 17: val_accuracy did not improve from 0.48535\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 1.1142 - accuracy: 0.4631 - val_loss: 1.2795 - val_accuracy: 0.3994\n",
            "Epoch 18/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1065 - accuracy: 0.4565\n",
            "Epoch 18: val_accuracy improved from 0.48535 to 0.51367, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 24s 189ms/step - loss: 1.1065 - accuracy: 0.4565 - val_loss: 1.1228 - val_accuracy: 0.5137\n",
            "Epoch 19/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1706 - accuracy: 0.4392\n",
            "Epoch 19: val_accuracy did not improve from 0.51367\n",
            "128/128 [==============================] - 25s 197ms/step - loss: 1.1706 - accuracy: 0.4392 - val_loss: 1.1402 - val_accuracy: 0.4443\n",
            "Epoch 20/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1063 - accuracy: 0.4641\n",
            "Epoch 20: val_accuracy did not improve from 0.51367\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 1.1063 - accuracy: 0.4641 - val_loss: 1.1188 - val_accuracy: 0.4512\n",
            "Epoch 21/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1005 - accuracy: 0.4685\n",
            "Epoch 21: val_accuracy improved from 0.51367 to 0.53418, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 1.1005 - accuracy: 0.4685 - val_loss: 0.9546 - val_accuracy: 0.5342\n",
            "Epoch 22/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0699 - accuracy: 0.4668\n",
            "Epoch 22: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 1.0699 - accuracy: 0.4668 - val_loss: 1.2075 - val_accuracy: 0.4453\n",
            "Epoch 23/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0381 - accuracy: 0.4761\n",
            "Epoch 23: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 1.0381 - accuracy: 0.4761 - val_loss: 1.0194 - val_accuracy: 0.5146\n",
            "Epoch 24/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1337 - accuracy: 0.4458\n",
            "Epoch 24: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 1.1337 - accuracy: 0.4458 - val_loss: 1.1494 - val_accuracy: 0.4639\n",
            "Epoch 25/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0664 - accuracy: 0.4751\n",
            "Epoch 25: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 1.0664 - accuracy: 0.4751 - val_loss: 1.0270 - val_accuracy: 0.5117\n",
            "Epoch 26/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0094 - accuracy: 0.5061\n",
            "Epoch 26: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 1.0094 - accuracy: 0.5061 - val_loss: 1.4152 - val_accuracy: 0.2949\n",
            "Epoch 27/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0913 - accuracy: 0.4705\n",
            "Epoch 27: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 31s 239ms/step - loss: 1.0913 - accuracy: 0.4705 - val_loss: 1.0563 - val_accuracy: 0.5000\n",
            "Epoch 28/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0326 - accuracy: 0.4792\n",
            "Epoch 28: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 182ms/step - loss: 1.0326 - accuracy: 0.4792 - val_loss: 1.6497 - val_accuracy: 0.2363\n",
            "Epoch 29/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0019 - accuracy: 0.4927\n",
            "Epoch 29: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 24s 187ms/step - loss: 1.0019 - accuracy: 0.4927 - val_loss: 1.0643 - val_accuracy: 0.4785\n",
            "Epoch 30/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.4832\n",
            "Epoch 30: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 1.0530 - accuracy: 0.4832 - val_loss: 1.1239 - val_accuracy: 0.5156\n",
            "Epoch 31/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0073 - accuracy: 0.4858\n",
            "Epoch 31: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 25s 193ms/step - loss: 1.0073 - accuracy: 0.4858 - val_loss: 1.0439 - val_accuracy: 0.5146\n",
            "Epoch 32/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9952 - accuracy: 0.4980\n",
            "Epoch 32: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 25s 190ms/step - loss: 0.9952 - accuracy: 0.4980 - val_loss: 1.1150 - val_accuracy: 0.4727\n",
            "Epoch 33/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9390 - accuracy: 0.5068\n",
            "Epoch 33: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 24s 186ms/step - loss: 0.9390 - accuracy: 0.5068 - val_loss: 1.0029 - val_accuracy: 0.5283\n",
            "Epoch 34/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9984 - accuracy: 0.4944\n",
            "Epoch 34: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 0.9984 - accuracy: 0.4944 - val_loss: 1.1834 - val_accuracy: 0.4424\n",
            "Epoch 35/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8999 - accuracy: 0.5137\n",
            "Epoch 35: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8999 - accuracy: 0.5137 - val_loss: 1.2799 - val_accuracy: 0.3574\n",
            "Epoch 36/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9588 - accuracy: 0.5056\n",
            "Epoch 36: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.9588 - accuracy: 0.5056 - val_loss: 1.0386 - val_accuracy: 0.5146\n",
            "Epoch 37/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9152 - accuracy: 0.5042\n",
            "Epoch 37: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.9152 - accuracy: 0.5042 - val_loss: 1.0914 - val_accuracy: 0.4941\n",
            "Epoch 38/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9714 - accuracy: 0.4993\n",
            "Epoch 38: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 0.9714 - accuracy: 0.4993 - val_loss: 1.0413 - val_accuracy: 0.5215\n",
            "Epoch 39/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9807 - accuracy: 0.5061\n",
            "Epoch 39: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 24s 185ms/step - loss: 0.9807 - accuracy: 0.5061 - val_loss: 1.3693 - val_accuracy: 0.3232\n",
            "Epoch 40/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9503 - accuracy: 0.5024\n",
            "Epoch 40: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 24s 188ms/step - loss: 0.9503 - accuracy: 0.5024 - val_loss: 2.0415 - val_accuracy: 0.0977\n",
            "Epoch 41/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9369 - accuracy: 0.5005\n",
            "Epoch 41: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 25s 198ms/step - loss: 0.9369 - accuracy: 0.5005 - val_loss: 1.4385 - val_accuracy: 0.2891\n",
            "Epoch 42/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9925 - accuracy: 0.4900\n",
            "Epoch 42: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.9925 - accuracy: 0.4900 - val_loss: 1.4124 - val_accuracy: 0.5127\n",
            "Epoch 43/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9043 - accuracy: 0.5093\n",
            "Epoch 43: val_accuracy did not improve from 0.53418\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 0.9043 - accuracy: 0.5093 - val_loss: 1.4314 - val_accuracy: 0.2871\n",
            "Epoch 44/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9244 - accuracy: 0.5132\n",
            "Epoch 44: val_accuracy improved from 0.53418 to 0.53906, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 25s 199ms/step - loss: 0.9244 - accuracy: 0.5132 - val_loss: 0.9598 - val_accuracy: 0.5391\n",
            "Epoch 45/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9182 - accuracy: 0.5098\n",
            "Epoch 45: val_accuracy did not improve from 0.53906\n",
            "128/128 [==============================] - 25s 196ms/step - loss: 0.9182 - accuracy: 0.5098 - val_loss: 1.5792 - val_accuracy: 0.2393\n",
            "Epoch 46/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9528 - accuracy: 0.5024\n",
            "Epoch 46: val_accuracy improved from 0.53906 to 0.54102, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.9528 - accuracy: 0.5024 - val_loss: 0.9110 - val_accuracy: 0.5410\n",
            "Epoch 47/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9022 - accuracy: 0.5166\n",
            "Epoch 47: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.9022 - accuracy: 0.5166 - val_loss: 1.9818 - val_accuracy: 0.1826\n",
            "Epoch 48/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8985 - accuracy: 0.5110\n",
            "Epoch 48: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 24s 186ms/step - loss: 0.8985 - accuracy: 0.5110 - val_loss: 1.3426 - val_accuracy: 0.3594\n",
            "Epoch 49/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8396 - accuracy: 0.5146\n",
            "Epoch 49: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 25s 197ms/step - loss: 0.8396 - accuracy: 0.5146 - val_loss: 1.0307 - val_accuracy: 0.4590\n",
            "Epoch 50/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8461 - accuracy: 0.5186\n",
            "Epoch 50: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8461 - accuracy: 0.5186 - val_loss: 1.0660 - val_accuracy: 0.4688\n",
            "Epoch 51/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8717 - accuracy: 0.5120\n",
            "Epoch 51: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.8717 - accuracy: 0.5120 - val_loss: 1.1893 - val_accuracy: 0.3877\n",
            "Epoch 52/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8711 - accuracy: 0.5276\n",
            "Epoch 52: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8711 - accuracy: 0.5276 - val_loss: 1.2657 - val_accuracy: 0.3848\n",
            "Epoch 53/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8952 - accuracy: 0.5217\n",
            "Epoch 53: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8952 - accuracy: 0.5217 - val_loss: 1.1997 - val_accuracy: 0.4248\n",
            "Epoch 54/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8614 - accuracy: 0.5188\n",
            "Epoch 54: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 24s 187ms/step - loss: 0.8614 - accuracy: 0.5188 - val_loss: 1.1180 - val_accuracy: 0.4795\n",
            "Epoch 55/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8624 - accuracy: 0.5225\n",
            "Epoch 55: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 23s 182ms/step - loss: 0.8624 - accuracy: 0.5225 - val_loss: 1.2024 - val_accuracy: 0.4082\n",
            "Epoch 56/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8653 - accuracy: 0.5215\n",
            "Epoch 56: val_accuracy did not improve from 0.54102\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 0.8653 - accuracy: 0.5215 - val_loss: 0.9946 - val_accuracy: 0.5205\n",
            "Epoch 57/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8231 - accuracy: 0.5244\n",
            "Epoch 57: val_accuracy improved from 0.54102 to 0.54395, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8231 - accuracy: 0.5244 - val_loss: 0.9159 - val_accuracy: 0.5439\n",
            "Epoch 58/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.5344\n",
            "Epoch 58: val_accuracy did not improve from 0.54395\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.7948 - accuracy: 0.5344 - val_loss: 1.0946 - val_accuracy: 0.4678\n",
            "Epoch 59/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9189 - accuracy: 0.5254\n",
            "Epoch 59: val_accuracy improved from 0.54395 to 0.55273, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.9189 - accuracy: 0.5254 - val_loss: 0.9344 - val_accuracy: 0.5527\n",
            "Epoch 60/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8311 - accuracy: 0.5342\n",
            "Epoch 60: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8311 - accuracy: 0.5342 - val_loss: 0.9190 - val_accuracy: 0.5420\n",
            "Epoch 61/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8863 - accuracy: 0.5249\n",
            "Epoch 61: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8863 - accuracy: 0.5249 - val_loss: 1.0253 - val_accuracy: 0.5117\n",
            "Epoch 62/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8380 - accuracy: 0.5215\n",
            "Epoch 62: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 182ms/step - loss: 0.8380 - accuracy: 0.5215 - val_loss: 1.0717 - val_accuracy: 0.4980\n",
            "Epoch 63/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8043 - accuracy: 0.5437\n",
            "Epoch 63: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 24s 186ms/step - loss: 0.8043 - accuracy: 0.5437 - val_loss: 1.6283 - val_accuracy: 0.1582\n",
            "Epoch 64/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8338 - accuracy: 0.5374\n",
            "Epoch 64: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 24s 188ms/step - loss: 0.8338 - accuracy: 0.5374 - val_loss: 1.1513 - val_accuracy: 0.3887\n",
            "Epoch 65/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.5298\n",
            "Epoch 65: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 0.9142 - accuracy: 0.5298 - val_loss: 0.9955 - val_accuracy: 0.5283\n",
            "Epoch 66/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.5254\n",
            "Epoch 66: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 26s 200ms/step - loss: 0.8234 - accuracy: 0.5254 - val_loss: 1.1440 - val_accuracy: 0.4395\n",
            "Epoch 67/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8363 - accuracy: 0.5293\n",
            "Epoch 67: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8363 - accuracy: 0.5293 - val_loss: 1.2838 - val_accuracy: 0.3652\n",
            "Epoch 68/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8187 - accuracy: 0.5486\n",
            "Epoch 68: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 178ms/step - loss: 0.8187 - accuracy: 0.5486 - val_loss: 1.5630 - val_accuracy: 0.2471\n",
            "Epoch 69/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8621 - accuracy: 0.5317\n",
            "Epoch 69: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8621 - accuracy: 0.5317 - val_loss: 1.2717 - val_accuracy: 0.4092\n",
            "Epoch 70/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8454 - accuracy: 0.5315\n",
            "Epoch 70: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8454 - accuracy: 0.5315 - val_loss: 1.6677 - val_accuracy: 0.2363\n",
            "Epoch 71/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7786 - accuracy: 0.5396\n",
            "Epoch 71: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7786 - accuracy: 0.5396 - val_loss: 0.9761 - val_accuracy: 0.5215\n",
            "Epoch 72/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8128 - accuracy: 0.5361\n",
            "Epoch 72: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 182ms/step - loss: 0.8128 - accuracy: 0.5361 - val_loss: 1.0253 - val_accuracy: 0.5244\n",
            "Epoch 73/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8463 - accuracy: 0.5254\n",
            "Epoch 73: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.8463 - accuracy: 0.5254 - val_loss: 1.3050 - val_accuracy: 0.3643\n",
            "Epoch 74/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7998 - accuracy: 0.5366\n",
            "Epoch 74: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 24s 187ms/step - loss: 0.7998 - accuracy: 0.5366 - val_loss: 1.3195 - val_accuracy: 0.3994\n",
            "Epoch 75/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.5508\n",
            "Epoch 75: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 24s 190ms/step - loss: 0.8063 - accuracy: 0.5508 - val_loss: 1.1659 - val_accuracy: 0.4180\n",
            "Epoch 76/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7905 - accuracy: 0.5330\n",
            "Epoch 76: val_accuracy improved from 0.55273 to 0.55762, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.7905 - accuracy: 0.5330 - val_loss: 0.9050 - val_accuracy: 0.5576\n",
            "Epoch 77/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7486 - accuracy: 0.5496\n",
            "Epoch 77: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7486 - accuracy: 0.5496 - val_loss: 1.7029 - val_accuracy: 0.1592\n",
            "Epoch 78/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8136 - accuracy: 0.5403\n",
            "Epoch 78: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.8136 - accuracy: 0.5403 - val_loss: 0.8830 - val_accuracy: 0.5283\n",
            "Epoch 79/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8407 - accuracy: 0.5222\n",
            "Epoch 79: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 26s 202ms/step - loss: 0.8407 - accuracy: 0.5222 - val_loss: 0.9380 - val_accuracy: 0.5508\n",
            "Epoch 80/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7900 - accuracy: 0.5425\n",
            "Epoch 80: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7900 - accuracy: 0.5425 - val_loss: 0.9469 - val_accuracy: 0.5234\n",
            "Epoch 81/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8287 - accuracy: 0.5381\n",
            "Epoch 81: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.8287 - accuracy: 0.5381 - val_loss: 0.8884 - val_accuracy: 0.5576\n",
            "Epoch 82/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7841 - accuracy: 0.5544\n",
            "Epoch 82: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 24s 189ms/step - loss: 0.7841 - accuracy: 0.5544 - val_loss: 1.3122 - val_accuracy: 0.3506\n",
            "Epoch 83/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8081 - accuracy: 0.5391\n",
            "Epoch 83: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 25s 191ms/step - loss: 0.8081 - accuracy: 0.5391 - val_loss: 0.9774 - val_accuracy: 0.5166\n",
            "Epoch 84/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.5178\n",
            "Epoch 84: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8836 - accuracy: 0.5178 - val_loss: 1.0252 - val_accuracy: 0.4883\n",
            "Epoch 85/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7887 - accuracy: 0.5518\n",
            "Epoch 85: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7887 - accuracy: 0.5518 - val_loss: 1.8146 - val_accuracy: 0.1670\n",
            "Epoch 86/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8075 - accuracy: 0.5466\n",
            "Epoch 86: val_accuracy improved from 0.55762 to 0.57324, saving model to best_model_1.h5\n",
            "128/128 [==============================] - 25s 192ms/step - loss: 0.8075 - accuracy: 0.5466 - val_loss: 0.8648 - val_accuracy: 0.5732\n",
            "Epoch 87/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.5491\n",
            "Epoch 87: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7933 - accuracy: 0.5491 - val_loss: 1.1953 - val_accuracy: 0.4209\n",
            "Epoch 88/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7377 - accuracy: 0.5488\n",
            "Epoch 88: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 24s 188ms/step - loss: 0.7377 - accuracy: 0.5488 - val_loss: 1.3499 - val_accuracy: 0.3262\n",
            "Epoch 89/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8113 - accuracy: 0.5403\n",
            "Epoch 89: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 24s 190ms/step - loss: 0.8113 - accuracy: 0.5403 - val_loss: 1.0849 - val_accuracy: 0.4746\n",
            "Epoch 90/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7868 - accuracy: 0.5410\n",
            "Epoch 90: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7868 - accuracy: 0.5410 - val_loss: 0.9532 - val_accuracy: 0.5332\n",
            "Epoch 91/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8426 - accuracy: 0.5249\n",
            "Epoch 91: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8426 - accuracy: 0.5249 - val_loss: 0.9940 - val_accuracy: 0.5342\n",
            "Epoch 92/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.5425\n",
            "Epoch 92: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.8014 - accuracy: 0.5425 - val_loss: 0.9269 - val_accuracy: 0.5557\n",
            "Epoch 93/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7708 - accuracy: 0.5481\n",
            "Epoch 93: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 181ms/step - loss: 0.7708 - accuracy: 0.5481 - val_loss: 0.9091 - val_accuracy: 0.5576\n",
            "Epoch 94/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.5476\n",
            "Epoch 94: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 180ms/step - loss: 0.7912 - accuracy: 0.5476 - val_loss: 0.8826 - val_accuracy: 0.5723\n",
            "Epoch 95/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7731 - accuracy: 0.5405\n",
            "Epoch 95: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 23s 179ms/step - loss: 0.7731 - accuracy: 0.5405 - val_loss: 1.1158 - val_accuracy: 0.5273\n",
            "Epoch 96/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7919 - accuracy: 0.5549\n",
            "Epoch 96: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 24s 188ms/step - loss: 0.7919 - accuracy: 0.5549 - val_loss: 1.1581 - val_accuracy: 0.4316\n",
            "Epoch 97/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7510 - accuracy: 0.5515\n",
            "Epoch 97: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 24s 189ms/step - loss: 0.7510 - accuracy: 0.5515 - val_loss: 1.0537 - val_accuracy: 0.4824\n",
            "Epoch 98/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7659 - accuracy: 0.5459\n",
            "Epoch 98: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 26s 199ms/step - loss: 0.7659 - accuracy: 0.5459 - val_loss: 0.9280 - val_accuracy: 0.5449\n",
            "Epoch 99/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7964 - accuracy: 0.5549\n",
            "Epoch 99: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 26s 204ms/step - loss: 0.7964 - accuracy: 0.5549 - val_loss: 0.9922 - val_accuracy: 0.4971\n",
            "Epoch 100/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7631 - accuracy: 0.5586\n",
            "Epoch 100: val_accuracy did not improve from 0.57324\n",
            "128/128 [==============================] - 24s 184ms/step - loss: 0.7631 - accuracy: 0.5586 - val_loss: 0.8923 - val_accuracy: 0.5547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model1.evaluate(test_images) # 1st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAs0KIVIojmx",
        "outputId": "77bc2df3-93de-41f4-cd81-8dad71ba4a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 4s 102ms/step - loss: 0.8546 - accuracy: 0.5734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.evaluate(train_images) , model1.evaluate(validation_images) #1st"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AgIwuBgonH-",
        "outputId": "b2d932a4-b05c-4109-ccf0-f18efa337984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128/128 [==============================] - 14s 112ms/step - loss: 0.8162 - accuracy: 0.6047\n",
            "32/32 [==============================] - 5s 153ms/step - loss: 0.8608 - accuracy: 0.5791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.8161712288856506, 0.604736328125], [0.8607696294784546, 0.5791015625])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_test_labels = test_images.classes\n",
        "test_pred = model1.predict(test_images)\n",
        "\n",
        "test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "confusion_Matrix = confusion_matrix(true_test_labels, test_pred_classes)\n",
        "sns.heatmap(confusion_Matrix, annot=True, cmap='Blues',fmt='.3g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "mwBOGQUKoscG",
        "outputId": "ce2b027c-54e4-43c3-9b28-e1ba5591222c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 4s 109ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_report = classification_report(true_test_labels, test_pred_classes)\n",
        "print(test_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9O151ZeFZZL",
        "outputId": "d91b88bc-89a1-4c7d-ed02-4421ff431bf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.19      0.17       201\n",
            "           1       0.04      0.17      0.07         6\n",
            "           2       0.52      0.47      0.49       643\n",
            "           3       0.38      0.37      0.37       430\n",
            "\n",
            "    accuracy                           0.39      1280\n",
            "   macro avg       0.27      0.30      0.28      1280\n",
            "weighted avg       0.41      0.39      0.40      1280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model2 with slightly different order: dropout to 0.2 too much loss"
      ],
      "metadata": {
        "id": "cZyP-ImuFZQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = models.Sequential()\n",
        "model2.add(layers.Conv2D(128, (3, 3), padding='same', input_shape=(176, 176, 1)))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(layers.Activation(\"relu\"))\n",
        "model2.add(layers.Dropout(0.2))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(layers.Conv2D(64,(3, 3),  padding='same'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(layers.Activation(\"relu\"))\n",
        "model2.add(layers.Dropout(0.2))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(layers.Conv2D(32,(3, 3),  padding='same'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(layers.Activation(\"relu\"))\n",
        "model2.add(layers.Dropout(0.2))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(layers.Conv2D(16,(3, 3),  padding='same'))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(layers.Activation(\"relu\"))\n",
        "model2.add(layers.Dropout(0.2))\n",
        "model2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model2.add(layers.Flatten())\n",
        "model2.add(layers.Dense(512))\n",
        "model2.add(layers.Dropout(0.2))\n",
        "model2.add(layers.Dense(4, activation='softmax'))\n",
        "model2.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "aTm0bnfwduKj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIUlOLt6gwcF",
        "outputId": "1d725f4a-2880-496f-9a79-b6a63cbaf753"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 176, 176, 128)     1280      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 176, 176, 128)     512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 176, 176, 128)     0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 176, 176, 128)     0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 88, 88, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 88, 88, 64)        73792     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 88, 88, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 88, 88, 64)        0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 88, 88, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 44, 44, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 44, 44, 32)        18464     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 44, 44, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 44, 44, 32)        0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 44, 44, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 22, 22, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 22, 22, 16)        4624      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 22, 22, 16)        64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 22, 22, 16)        0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 22, 22, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 11, 11, 16)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1936)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               991744    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1092916 (4.17 MB)\n",
            "Trainable params: 1092436 (4.17 MB)\n",
            "Non-trainable params: 480 (1.88 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.load_weights('/content/shuffled_model2_1.h5')"
      ],
      "metadata": {
        "id": "Oq3Lf9EsgzQK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint('shuffled_model2_1.h5', save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=2)\n",
        "history2 = model2.fit(train_images, validation_data=validation_images ,callbacks=[model_checkpoint], class_weight=class_weights_dict , epochs= 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AREQD1-shSMD",
        "outputId": "0f6e84e8-c0e4-4ae4-8902-ea82b70cb965"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 7.6886 - accuracy: 0.2986\n",
            "Epoch 1: val_accuracy improved from -inf to 0.00781, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 47s 275ms/step - loss: 7.6886 - accuracy: 0.2986 - val_loss: 2.1067 - val_accuracy: 0.0078\n",
            "Epoch 2/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 2.9924 - accuracy: 0.3467\n",
            "Epoch 2: val_accuracy improved from 0.00781 to 0.34570, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 33s 261ms/step - loss: 2.9924 - accuracy: 0.3467 - val_loss: 1.5175 - val_accuracy: 0.3457\n",
            "Epoch 3/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 2.7499 - accuracy: 0.3328\n",
            "Epoch 3: val_accuracy improved from 0.34570 to 0.51172, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 34s 262ms/step - loss: 2.7499 - accuracy: 0.3328 - val_loss: 1.2444 - val_accuracy: 0.5117\n",
            "Epoch 4/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.7917 - accuracy: 0.3918\n",
            "Epoch 4: val_accuracy did not improve from 0.51172\n",
            "128/128 [==============================] - 33s 259ms/step - loss: 1.7917 - accuracy: 0.3918 - val_loss: 1.4209 - val_accuracy: 0.1348\n",
            "Epoch 5/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.6948 - accuracy: 0.3965\n",
            "Epoch 5: val_accuracy did not improve from 0.51172\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 1.6948 - accuracy: 0.3965 - val_loss: 1.2462 - val_accuracy: 0.4287\n",
            "Epoch 6/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.3484 - accuracy: 0.4224\n",
            "Epoch 6: val_accuracy did not improve from 0.51172\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 1.3484 - accuracy: 0.4224 - val_loss: 1.0575 - val_accuracy: 0.4795\n",
            "Epoch 7/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.3156 - accuracy: 0.4426\n",
            "Epoch 7: val_accuracy did not improve from 0.51172\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 1.3156 - accuracy: 0.4426 - val_loss: 1.6034 - val_accuracy: 0.0488\n",
            "Epoch 8/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.2138 - accuracy: 0.4390\n",
            "Epoch 8: val_accuracy improved from 0.51172 to 0.52148, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 1.2138 - accuracy: 0.4390 - val_loss: 1.1732 - val_accuracy: 0.5215\n",
            "Epoch 9/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1910 - accuracy: 0.4478\n",
            "Epoch 9: val_accuracy did not improve from 0.52148\n",
            "128/128 [==============================] - 32s 246ms/step - loss: 1.1910 - accuracy: 0.4478 - val_loss: 1.1400 - val_accuracy: 0.3604\n",
            "Epoch 10/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1169 - accuracy: 0.4609\n",
            "Epoch 10: val_accuracy did not improve from 0.52148\n",
            "128/128 [==============================] - 33s 256ms/step - loss: 1.1169 - accuracy: 0.4609 - val_loss: 1.0712 - val_accuracy: 0.4854\n",
            "Epoch 11/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1554 - accuracy: 0.4534\n",
            "Epoch 11: val_accuracy improved from 0.52148 to 0.52344, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 1.1554 - accuracy: 0.4534 - val_loss: 1.0604 - val_accuracy: 0.5234\n",
            "Epoch 12/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.4839\n",
            "Epoch 12: val_accuracy did not improve from 0.52344\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 1.0313 - accuracy: 0.4839 - val_loss: 1.0492 - val_accuracy: 0.5117\n",
            "Epoch 13/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9595 - accuracy: 0.4893\n",
            "Epoch 13: val_accuracy improved from 0.52344 to 0.53906, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.9595 - accuracy: 0.4893 - val_loss: 0.9870 - val_accuracy: 0.5391\n",
            "Epoch 14/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9800 - accuracy: 0.5024\n",
            "Epoch 14: val_accuracy did not improve from 0.53906\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 0.9800 - accuracy: 0.5024 - val_loss: 1.3497 - val_accuracy: 0.2871\n",
            "Epoch 15/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9806 - accuracy: 0.4937\n",
            "Epoch 15: val_accuracy improved from 0.53906 to 0.55273, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.9806 - accuracy: 0.4937 - val_loss: 0.9821 - val_accuracy: 0.5527\n",
            "Epoch 16/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9697 - accuracy: 0.5098\n",
            "Epoch 16: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.9697 - accuracy: 0.5098 - val_loss: 1.3650 - val_accuracy: 0.2822\n",
            "Epoch 17/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.1029 - accuracy: 0.4663\n",
            "Epoch 17: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 1.1029 - accuracy: 0.4663 - val_loss: 1.1796 - val_accuracy: 0.3447\n",
            "Epoch 18/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9616 - accuracy: 0.5000\n",
            "Epoch 18: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.9616 - accuracy: 0.5000 - val_loss: 1.2313 - val_accuracy: 0.3662\n",
            "Epoch 19/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9877 - accuracy: 0.5059\n",
            "Epoch 19: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 34s 261ms/step - loss: 0.9877 - accuracy: 0.5059 - val_loss: 1.2880 - val_accuracy: 0.4326\n",
            "Epoch 20/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9620 - accuracy: 0.5066\n",
            "Epoch 20: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.9620 - accuracy: 0.5066 - val_loss: 1.7561 - val_accuracy: 0.1592\n",
            "Epoch 21/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8958 - accuracy: 0.5188\n",
            "Epoch 21: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.8958 - accuracy: 0.5188 - val_loss: 1.0481 - val_accuracy: 0.4854\n",
            "Epoch 22/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8269 - accuracy: 0.5332\n",
            "Epoch 22: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.8269 - accuracy: 0.5332 - val_loss: 1.0173 - val_accuracy: 0.4971\n",
            "Epoch 23/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.9146 - accuracy: 0.5195\n",
            "Epoch 23: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 254ms/step - loss: 0.9146 - accuracy: 0.5195 - val_loss: 0.9618 - val_accuracy: 0.5205\n",
            "Epoch 24/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8927 - accuracy: 0.5227\n",
            "Epoch 24: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.8927 - accuracy: 0.5227 - val_loss: 1.2904 - val_accuracy: 0.3154\n",
            "Epoch 25/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7996 - accuracy: 0.5457\n",
            "Epoch 25: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7996 - accuracy: 0.5457 - val_loss: 0.9820 - val_accuracy: 0.5391\n",
            "Epoch 26/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8583 - accuracy: 0.5295\n",
            "Epoch 26: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.8583 - accuracy: 0.5295 - val_loss: 1.2131 - val_accuracy: 0.4551\n",
            "Epoch 27/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 1.0044 - accuracy: 0.5081\n",
            "Epoch 27: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 32s 246ms/step - loss: 1.0044 - accuracy: 0.5081 - val_loss: 1.0529 - val_accuracy: 0.4668\n",
            "Epoch 28/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8132 - accuracy: 0.5339\n",
            "Epoch 28: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 256ms/step - loss: 0.8132 - accuracy: 0.5339 - val_loss: 1.0386 - val_accuracy: 0.5049\n",
            "Epoch 29/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8357 - accuracy: 0.5332\n",
            "Epoch 29: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 261ms/step - loss: 0.8357 - accuracy: 0.5332 - val_loss: 1.2410 - val_accuracy: 0.3340\n",
            "Epoch 30/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8190 - accuracy: 0.5249\n",
            "Epoch 30: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.8190 - accuracy: 0.5249 - val_loss: 1.1762 - val_accuracy: 0.3936\n",
            "Epoch 31/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8963 - accuracy: 0.5254\n",
            "Epoch 31: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.8963 - accuracy: 0.5254 - val_loss: 1.0549 - val_accuracy: 0.4746\n",
            "Epoch 32/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7795 - accuracy: 0.5339\n",
            "Epoch 32: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 0.7795 - accuracy: 0.5339 - val_loss: 1.1428 - val_accuracy: 0.5225\n",
            "Epoch 33/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8635 - accuracy: 0.5322\n",
            "Epoch 33: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.8635 - accuracy: 0.5322 - val_loss: 1.4778 - val_accuracy: 0.2100\n",
            "Epoch 34/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.5483\n",
            "Epoch 34: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7850 - accuracy: 0.5483 - val_loss: 0.9126 - val_accuracy: 0.5488\n",
            "Epoch 35/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8244 - accuracy: 0.5493\n",
            "Epoch 35: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.8244 - accuracy: 0.5493 - val_loss: 1.6301 - val_accuracy: 0.2275\n",
            "Epoch 36/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8867 - accuracy: 0.5332\n",
            "Epoch 36: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 253ms/step - loss: 0.8867 - accuracy: 0.5332 - val_loss: 1.2387 - val_accuracy: 0.3760\n",
            "Epoch 37/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7601 - accuracy: 0.5549\n",
            "Epoch 37: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 0.7601 - accuracy: 0.5549 - val_loss: 1.0619 - val_accuracy: 0.4346\n",
            "Epoch 38/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7978 - accuracy: 0.5488\n",
            "Epoch 38: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 33s 254ms/step - loss: 0.7978 - accuracy: 0.5488 - val_loss: 1.1697 - val_accuracy: 0.3984\n",
            "Epoch 39/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7083 - accuracy: 0.5547\n",
            "Epoch 39: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7083 - accuracy: 0.5547 - val_loss: 1.2151 - val_accuracy: 0.3613\n",
            "Epoch 40/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7825 - accuracy: 0.5498\n",
            "Epoch 40: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7825 - accuracy: 0.5498 - val_loss: 1.0130 - val_accuracy: 0.5020\n",
            "Epoch 41/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7247 - accuracy: 0.5520\n",
            "Epoch 41: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7247 - accuracy: 0.5520 - val_loss: 1.3786 - val_accuracy: 0.2773\n",
            "Epoch 42/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8059 - accuracy: 0.5444\n",
            "Epoch 42: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 32s 251ms/step - loss: 0.8059 - accuracy: 0.5444 - val_loss: 1.2798 - val_accuracy: 0.3877\n",
            "Epoch 43/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7833 - accuracy: 0.5488\n",
            "Epoch 43: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.7833 - accuracy: 0.5488 - val_loss: 0.9134 - val_accuracy: 0.5371\n",
            "Epoch 44/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7291 - accuracy: 0.5647\n",
            "Epoch 44: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7291 - accuracy: 0.5647 - val_loss: 1.0274 - val_accuracy: 0.4834\n",
            "Epoch 45/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7280 - accuracy: 0.5574\n",
            "Epoch 45: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.7280 - accuracy: 0.5574 - val_loss: 0.9194 - val_accuracy: 0.5488\n",
            "Epoch 46/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.8109 - accuracy: 0.5547\n",
            "Epoch 46: val_accuracy did not improve from 0.55273\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.8109 - accuracy: 0.5547 - val_loss: 0.9142 - val_accuracy: 0.5342\n",
            "Epoch 47/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7417 - accuracy: 0.5522\n",
            "Epoch 47: val_accuracy improved from 0.55273 to 0.55762, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.7417 - accuracy: 0.5522 - val_loss: 0.9074 - val_accuracy: 0.5576\n",
            "Epoch 48/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7075 - accuracy: 0.5686\n",
            "Epoch 48: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 0.7075 - accuracy: 0.5686 - val_loss: 1.4949 - val_accuracy: 0.3008\n",
            "Epoch 49/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7151 - accuracy: 0.5754\n",
            "Epoch 49: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.7151 - accuracy: 0.5754 - val_loss: 1.1236 - val_accuracy: 0.4111\n",
            "Epoch 50/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7219 - accuracy: 0.5676\n",
            "Epoch 50: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 34s 261ms/step - loss: 0.7219 - accuracy: 0.5676 - val_loss: 1.1758 - val_accuracy: 0.3877\n",
            "Epoch 51/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6967 - accuracy: 0.5696\n",
            "Epoch 51: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.6967 - accuracy: 0.5696 - val_loss: 0.9872 - val_accuracy: 0.5186\n",
            "Epoch 52/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.5540\n",
            "Epoch 52: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.7879 - accuracy: 0.5540 - val_loss: 1.1482 - val_accuracy: 0.3770\n",
            "Epoch 53/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7070 - accuracy: 0.5693\n",
            "Epoch 53: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 32s 253ms/step - loss: 0.7070 - accuracy: 0.5693 - val_loss: 1.8093 - val_accuracy: 0.2207\n",
            "Epoch 54/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7382 - accuracy: 0.5691\n",
            "Epoch 54: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7382 - accuracy: 0.5691 - val_loss: 1.4864 - val_accuracy: 0.3047\n",
            "Epoch 55/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.5754\n",
            "Epoch 55: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6983 - accuracy: 0.5754 - val_loss: 1.3221 - val_accuracy: 0.3525\n",
            "Epoch 56/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.5776\n",
            "Epoch 56: val_accuracy did not improve from 0.55762\n",
            "128/128 [==============================] - 32s 245ms/step - loss: 0.7185 - accuracy: 0.5776 - val_loss: 1.2021 - val_accuracy: 0.3691\n",
            "Epoch 57/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7252 - accuracy: 0.5757\n",
            "Epoch 57: val_accuracy improved from 0.55762 to 0.56445, saving model to shuffled_model2_1.h5\n",
            "128/128 [==============================] - 34s 262ms/step - loss: 0.7252 - accuracy: 0.5757 - val_loss: 0.9044 - val_accuracy: 0.5645\n",
            "Epoch 58/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7071 - accuracy: 0.5735\n",
            "Epoch 58: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.7071 - accuracy: 0.5735 - val_loss: 1.1450 - val_accuracy: 0.3984\n",
            "Epoch 59/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6901 - accuracy: 0.5728\n",
            "Epoch 59: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6901 - accuracy: 0.5728 - val_loss: 1.1211 - val_accuracy: 0.4297\n",
            "Epoch 60/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7101 - accuracy: 0.5693\n",
            "Epoch 60: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7101 - accuracy: 0.5693 - val_loss: 1.3358 - val_accuracy: 0.2979\n",
            "Epoch 61/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7222 - accuracy: 0.5815\n",
            "Epoch 61: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 246ms/step - loss: 0.7222 - accuracy: 0.5815 - val_loss: 1.6196 - val_accuracy: 0.2900\n",
            "Epoch 62/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7059 - accuracy: 0.5688\n",
            "Epoch 62: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.7059 - accuracy: 0.5688 - val_loss: 1.1762 - val_accuracy: 0.3701\n",
            "Epoch 63/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6963 - accuracy: 0.5959\n",
            "Epoch 63: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6963 - accuracy: 0.5959 - val_loss: 1.0762 - val_accuracy: 0.4395\n",
            "Epoch 64/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7262 - accuracy: 0.5801\n",
            "Epoch 64: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.7262 - accuracy: 0.5801 - val_loss: 1.5931 - val_accuracy: 0.3154\n",
            "Epoch 65/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.5845\n",
            "Epoch 65: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 34s 261ms/step - loss: 0.7093 - accuracy: 0.5845 - val_loss: 1.0322 - val_accuracy: 0.5576\n",
            "Epoch 66/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6850 - accuracy: 0.5698\n",
            "Epoch 66: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6850 - accuracy: 0.5698 - val_loss: 1.0569 - val_accuracy: 0.4521\n",
            "Epoch 67/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6545 - accuracy: 0.5864\n",
            "Epoch 67: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6545 - accuracy: 0.5864 - val_loss: 1.1326 - val_accuracy: 0.4072\n",
            "Epoch 68/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.5967\n",
            "Epoch 68: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6805 - accuracy: 0.5967 - val_loss: 1.5221 - val_accuracy: 0.2725\n",
            "Epoch 69/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7066 - accuracy: 0.5872\n",
            "Epoch 69: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.7066 - accuracy: 0.5872 - val_loss: 0.9880 - val_accuracy: 0.4766\n",
            "Epoch 70/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.5774\n",
            "Epoch 70: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 34s 264ms/step - loss: 0.7030 - accuracy: 0.5774 - val_loss: 1.2363 - val_accuracy: 0.3838\n",
            "Epoch 71/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.6006\n",
            "Epoch 71: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 0.6833 - accuracy: 0.6006 - val_loss: 1.2375 - val_accuracy: 0.3604\n",
            "Epoch 72/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6357 - accuracy: 0.6021\n",
            "Epoch 72: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 0.6357 - accuracy: 0.6021 - val_loss: 1.0715 - val_accuracy: 0.4404\n",
            "Epoch 73/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.5952\n",
            "Epoch 73: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 34s 262ms/step - loss: 0.6668 - accuracy: 0.5952 - val_loss: 1.5087 - val_accuracy: 0.3467\n",
            "Epoch 74/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.7488 - accuracy: 0.5828\n",
            "Epoch 74: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.7488 - accuracy: 0.5828 - val_loss: 0.9251 - val_accuracy: 0.5293\n",
            "Epoch 75/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.5989\n",
            "Epoch 75: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6792 - accuracy: 0.5989 - val_loss: 1.1195 - val_accuracy: 0.3730\n",
            "Epoch 76/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.6147\n",
            "Epoch 76: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 259ms/step - loss: 0.6435 - accuracy: 0.6147 - val_loss: 1.2611 - val_accuracy: 0.3447\n",
            "Epoch 77/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6848 - accuracy: 0.5869\n",
            "Epoch 77: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 0.6848 - accuracy: 0.5869 - val_loss: 1.5127 - val_accuracy: 0.2676\n",
            "Epoch 78/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.6021\n",
            "Epoch 78: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6774 - accuracy: 0.6021 - val_loss: 1.3211 - val_accuracy: 0.3193\n",
            "Epoch 79/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.6101\n",
            "Epoch 79: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6459 - accuracy: 0.6101 - val_loss: 1.2758 - val_accuracy: 0.3584\n",
            "Epoch 80/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6623 - accuracy: 0.5984\n",
            "Epoch 80: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6623 - accuracy: 0.5984 - val_loss: 1.1593 - val_accuracy: 0.3857\n",
            "Epoch 81/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6682 - accuracy: 0.6074\n",
            "Epoch 81: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 247ms/step - loss: 0.6682 - accuracy: 0.6074 - val_loss: 1.1521 - val_accuracy: 0.3730\n",
            "Epoch 82/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.6077\n",
            "Epoch 82: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 0.6569 - accuracy: 0.6077 - val_loss: 0.9923 - val_accuracy: 0.4834\n",
            "Epoch 83/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6184 - accuracy: 0.6196\n",
            "Epoch 83: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.6184 - accuracy: 0.6196 - val_loss: 0.9421 - val_accuracy: 0.5596\n",
            "Epoch 84/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6622 - accuracy: 0.6050\n",
            "Epoch 84: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6622 - accuracy: 0.6050 - val_loss: 1.2275 - val_accuracy: 0.3135\n",
            "Epoch 85/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6297 - accuracy: 0.6121\n",
            "Epoch 85: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6297 - accuracy: 0.6121 - val_loss: 0.8944 - val_accuracy: 0.5508\n",
            "Epoch 86/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6376 - accuracy: 0.6162\n",
            "Epoch 86: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 242ms/step - loss: 0.6376 - accuracy: 0.6162 - val_loss: 1.1133 - val_accuracy: 0.3984\n",
            "Epoch 87/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6242 - accuracy: 0.6162\n",
            "Epoch 87: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.6242 - accuracy: 0.6162 - val_loss: 1.0098 - val_accuracy: 0.4404\n",
            "Epoch 88/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.6121\n",
            "Epoch 88: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 260ms/step - loss: 0.6418 - accuracy: 0.6121 - val_loss: 1.3514 - val_accuracy: 0.3086\n",
            "Epoch 89/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.6035\n",
            "Epoch 89: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.6507 - accuracy: 0.6035 - val_loss: 1.1456 - val_accuracy: 0.3750\n",
            "Epoch 90/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.6206\n",
            "Epoch 90: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6203 - accuracy: 0.6206 - val_loss: 0.9998 - val_accuracy: 0.4990\n",
            "Epoch 91/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.6233\n",
            "Epoch 91: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6138 - accuracy: 0.6233 - val_loss: 1.1155 - val_accuracy: 0.3857\n",
            "Epoch 92/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6433 - accuracy: 0.6138\n",
            "Epoch 92: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.6433 - accuracy: 0.6138 - val_loss: 1.1294 - val_accuracy: 0.3779\n",
            "Epoch 93/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.6121\n",
            "Epoch 93: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 245ms/step - loss: 0.6355 - accuracy: 0.6121 - val_loss: 1.1126 - val_accuracy: 0.3857\n",
            "Epoch 94/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.6208\n",
            "Epoch 94: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 243ms/step - loss: 0.6241 - accuracy: 0.6208 - val_loss: 1.0249 - val_accuracy: 0.4492\n",
            "Epoch 95/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.6143\n",
            "Epoch 95: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 257ms/step - loss: 0.6186 - accuracy: 0.6143 - val_loss: 0.9750 - val_accuracy: 0.5010\n",
            "Epoch 96/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.6331\n",
            "Epoch 96: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 245ms/step - loss: 0.5993 - accuracy: 0.6331 - val_loss: 0.9206 - val_accuracy: 0.5410\n",
            "Epoch 97/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6107 - accuracy: 0.6362\n",
            "Epoch 97: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 31s 244ms/step - loss: 0.6107 - accuracy: 0.6362 - val_loss: 1.3470 - val_accuracy: 0.3076\n",
            "Epoch 98/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.6243\n",
            "Epoch 98: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 249ms/step - loss: 0.6279 - accuracy: 0.6243 - val_loss: 1.0936 - val_accuracy: 0.4150\n",
            "Epoch 99/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.6348\n",
            "Epoch 99: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 32s 248ms/step - loss: 0.6065 - accuracy: 0.6348 - val_loss: 0.9514 - val_accuracy: 0.5273\n",
            "Epoch 100/100\n",
            "128/128 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.6406\n",
            "Epoch 100: val_accuracy did not improve from 0.56445\n",
            "128/128 [==============================] - 33s 253ms/step - loss: 0.5986 - accuracy: 0.6406 - val_loss: 1.0561 - val_accuracy: 0.4170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model2.evaluate(test_images) # 1st"
      ],
      "metadata": {
        "id": "1RvK-BxghSJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d626a4-73bf-4bef-d318-275f9a06878f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 4s 97ms/step - loss: 0.9238 - accuracy: 0.5500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(train_images) , model2.evaluate(validation_images) #1st"
      ],
      "metadata": {
        "id": "1l_SiiX2hSGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039cae14-56d3-44c0-ec9c-813df85d0320"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128/128 [==============================] - 14s 108ms/step - loss: 0.8948 - accuracy: 0.5659\n",
            "32/32 [==============================] - 3s 94ms/step - loss: 0.9137 - accuracy: 0.5713\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.8947915434837341, 0.56591796875], [0.9136548042297363, 0.5712890625])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_test_labels = test_images.classes\n",
        "test_pred = model3.predict(test_images)\n",
        "\n",
        "test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "confusion_Matrix = confusion_matrix(true_test_labels, test_pred_classes)\n",
        "sns.heatmap(confusion_Matrix, annot=True, cmap='Blues',fmt='.3g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dV0u3oYzhSB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_report = classification_report(true_test_labels, test_pred_classes)\n",
        "print(test_report)"
      ],
      "metadata": {
        "id": "1ZgB_sK0isyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try changing the batch size and use smaller models"
      ],
      "metadata": {
        "id": "OVfOOVsJitxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOC5f2VriuYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUZGpD60iuoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpchbtVZiuzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 3 vgg style multiple conv2d with activation"
      ],
      "metadata": {
        "id": "YXawYChi8Efn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = models.Sequential()\n",
        "model3.add(layers.Conv2D(128, (3, 3), padding='same', input_shape=(176, 176, 1)))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(layers.Dropout(0.3))\n",
        "model3.add(layers.Activation(\"relu\"))\n",
        "model3.add(layers.Conv2D(64,(3, 3),  padding='same'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(layers.Dropout(0.3))\n",
        "model3.add(layers.Activation(\"relu\"))\n",
        "model3.add(layers.Conv2D(32,(3, 3),  padding='same'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(layers.Dropout(0.3))\n",
        "model3.add(layers.Activation(\"relu\"))\n",
        "model3.add(layers.Conv2D(16,(3, 3),  padding='same'))\n",
        "model3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(layers.Dropout(0.3))\n",
        "model3.add(layers.Activation(\"relu\"))\n",
        "model3.add(layers.Flatten())\n",
        "model3.add(layers.Dense(512))\n",
        "model3.add(layers.Dropout(0.3))\n",
        "model3.add(layers.Dense(4, activation='softmax'))\n",
        "model3.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "5H2cyAOtdtKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbNg369OjSBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "id": "SYPeMK3-jSV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.load_weights('/content/shuffled_model3_1.h5')"
      ],
      "metadata": {
        "id": "30Mq_87ejSWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint('shuffled_model3_1.h5', save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=2)\n",
        "history3 = model3.fit(train_images, validation_data=validation_images ,callbacks=[model_checkpoint], class_weight=class_weights_dict , epochs= 100)"
      ],
      "metadata": {
        "id": "nIjLM5zDjSWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model3.evaluate(test_images) # 1st"
      ],
      "metadata": {
        "id": "kEyzxIV4jSWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.evaluate(train_images) , model3.evaluate(validation_images) #1st"
      ],
      "metadata": {
        "id": "nw_Dg6QljSWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_test_labels = test_images.classes\n",
        "test_pred = model3.predict(test_images)\n",
        "\n",
        "test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "confusion_Matrix = confusion_matrix(true_test_labels, test_pred_classes)\n",
        "sns.heatmap(confusion_Matrix, annot=True, cmap='Blues',fmt='.3g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RBvWl31RjSWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_report = classification_report(true_test_labels, test_pred_classes)\n",
        "print(test_report)"
      ],
      "metadata": {
        "id": "ENRJZto2jSWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VAXm_5FDrC8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Azh6sxcrC5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yfkOG71prC3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P29H4vt4rC01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple model-4 , with less layers"
      ],
      "metadata": {
        "id": "NUrwDLPWrCdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = models.Sequential()\n",
        "model4.add(layers.Conv2D(128, (3, 3), padding='same',activation ='relu', input_shape=(176, 176, 1)))\n",
        "model4.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(layers.Dropout(0.2))\n",
        "model4.add(layers.Conv2D(64,(3, 3),  padding='same',activation ='relu'))\n",
        "model4.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(layers.Flatten())\n",
        "model4.add(layers.Dense(512, activation='relu'))\n",
        "model4.add(layers.Dropout(0.2))\n",
        "model4.add(layers.Dense(256, activation= 'relu'))\n",
        "model4.add(BatchNormalization())\n",
        "model4.add(layers.Dropout(0.2))\n",
        "model4.add(layers.Dense(4, activation='softmax'))\n",
        "model4.compile(optimizer= optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "lEEqsxC7SJgu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lceh2aXItDkI",
        "outputId": "8e0404fd-2992-4c1c-9285-ab0fc57889e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 176, 176, 128)     1280      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 88, 88, 128)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 88, 88, 128)       512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 88, 88, 128)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 88, 88, 64)        73792     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 44, 44, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 44, 44, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 123904)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               63439360  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 256)               1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63648580 (242.80 MB)\n",
            "Trainable params: 63647684 (242.80 MB)\n",
            "Non-trainable params: 896 (3.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.load_weights('/content/shuffled_model4_1-3.h5')"
      ],
      "metadata": {
        "id": "O0K0lk7f1WGD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = ModelCheckpoint('shuffled_model4_1-3.h5', save_best_only=True, save_weights_only=True, monitor='val_accuracy', mode='max', verbose=2)\n",
        "history4 = model4.fit(train_images, validation_data=validation_images ,callbacks=[model_checkpoint] ,class_weight= class_weights_dict, epochs= 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxeEa3q_tmN3",
        "outputId": "cafd08ee-59da-46d5-a26f-b6ce2ba7ff49"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3331 - accuracy: 0.8096\n",
            "Epoch 1: val_accuracy improved from -inf to 0.80078, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 29s 113ms/step - loss: 0.3331 - accuracy: 0.8096 - val_loss: 0.4674 - val_accuracy: 0.8008\n",
            "Epoch 2/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3232 - accuracy: 0.8052\n",
            "Epoch 2: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3232 - accuracy: 0.8052 - val_loss: 0.6544 - val_accuracy: 0.7188\n",
            "Epoch 3/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8103\n",
            "Epoch 3: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3121 - accuracy: 0.8103 - val_loss: 0.6881 - val_accuracy: 0.7100\n",
            "Epoch 4/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3125 - accuracy: 0.8171\n",
            "Epoch 4: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 27s 104ms/step - loss: 0.3125 - accuracy: 0.8171 - val_loss: 0.5165 - val_accuracy: 0.7783\n",
            "Epoch 5/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2972 - accuracy: 0.8198\n",
            "Epoch 5: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 27s 106ms/step - loss: 0.2972 - accuracy: 0.8198 - val_loss: 0.5725 - val_accuracy: 0.7549\n",
            "Epoch 6/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.5813 - accuracy: 0.7131\n",
            "Epoch 6: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 27s 107ms/step - loss: 0.5813 - accuracy: 0.7131 - val_loss: 1.5622 - val_accuracy: 0.5654\n",
            "Epoch 7/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.5645 - accuracy: 0.7026\n",
            "Epoch 7: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.5645 - accuracy: 0.7026 - val_loss: 0.7503 - val_accuracy: 0.6826\n",
            "Epoch 8/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.4165 - accuracy: 0.7798\n",
            "Epoch 8: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 26s 101ms/step - loss: 0.4165 - accuracy: 0.7798 - val_loss: 0.6084 - val_accuracy: 0.7344\n",
            "Epoch 9/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8081\n",
            "Epoch 9: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 27s 107ms/step - loss: 0.3195 - accuracy: 0.8081 - val_loss: 0.5063 - val_accuracy: 0.7783\n",
            "Epoch 10/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8201\n",
            "Epoch 10: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 28s 107ms/step - loss: 0.3024 - accuracy: 0.8201 - val_loss: 0.5079 - val_accuracy: 0.7783\n",
            "Epoch 11/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.8096\n",
            "Epoch 11: val_accuracy did not improve from 0.80078\n",
            "256/256 [==============================] - 26s 102ms/step - loss: 0.3313 - accuracy: 0.8096 - val_loss: 0.5489 - val_accuracy: 0.7568\n",
            "Epoch 12/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.8191\n",
            "Epoch 12: val_accuracy improved from 0.80078 to 0.80469, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.3341 - accuracy: 0.8191 - val_loss: 0.4486 - val_accuracy: 0.8047\n",
            "Epoch 13/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2778 - accuracy: 0.8333\n",
            "Epoch 13: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 27s 103ms/step - loss: 0.2778 - accuracy: 0.8333 - val_loss: 0.5240 - val_accuracy: 0.7744\n",
            "Epoch 14/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8289\n",
            "Epoch 14: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.3181 - accuracy: 0.8289 - val_loss: 0.5467 - val_accuracy: 0.7861\n",
            "Epoch 15/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3095 - accuracy: 0.8293\n",
            "Epoch 15: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.3095 - accuracy: 0.8293 - val_loss: 0.4634 - val_accuracy: 0.7881\n",
            "Epoch 16/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3380 - accuracy: 0.8240\n",
            "Epoch 16: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 26s 100ms/step - loss: 0.3380 - accuracy: 0.8240 - val_loss: 0.4750 - val_accuracy: 0.7812\n",
            "Epoch 17/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.7764\n",
            "Epoch 17: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 26s 101ms/step - loss: 0.4231 - accuracy: 0.7764 - val_loss: 0.6735 - val_accuracy: 0.7158\n",
            "Epoch 18/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8171\n",
            "Epoch 18: val_accuracy did not improve from 0.80469\n",
            "256/256 [==============================] - 27s 106ms/step - loss: 0.3090 - accuracy: 0.8171 - val_loss: 0.4748 - val_accuracy: 0.7871\n",
            "Epoch 19/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2735 - accuracy: 0.8376\n",
            "Epoch 19: val_accuracy improved from 0.80469 to 0.81348, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 29s 111ms/step - loss: 0.2735 - accuracy: 0.8376 - val_loss: 0.4501 - val_accuracy: 0.8135\n",
            "Epoch 20/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.8359\n",
            "Epoch 20: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 26s 102ms/step - loss: 0.2820 - accuracy: 0.8359 - val_loss: 0.5352 - val_accuracy: 0.7881\n",
            "Epoch 21/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2820 - accuracy: 0.8376\n",
            "Epoch 21: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 26s 101ms/step - loss: 0.2820 - accuracy: 0.8376 - val_loss: 0.4406 - val_accuracy: 0.8115\n",
            "Epoch 22/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.8394\n",
            "Epoch 22: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 27s 106ms/step - loss: 0.2788 - accuracy: 0.8394 - val_loss: 0.5639 - val_accuracy: 0.7803\n",
            "Epoch 23/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.8416\n",
            "Epoch 23: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3019 - accuracy: 0.8416 - val_loss: 1.6629 - val_accuracy: 0.5654\n",
            "Epoch 24/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8479\n",
            "Epoch 24: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3014 - accuracy: 0.8479 - val_loss: 0.5219 - val_accuracy: 0.7793\n",
            "Epoch 25/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.8352\n",
            "Epoch 25: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 27s 104ms/step - loss: 0.3270 - accuracy: 0.8352 - val_loss: 0.5869 - val_accuracy: 0.7422\n",
            "Epoch 26/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8340\n",
            "Epoch 26: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.3007 - accuracy: 0.8340 - val_loss: 0.6522 - val_accuracy: 0.7588\n",
            "Epoch 27/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8335\n",
            "Epoch 27: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.2866 - accuracy: 0.8335 - val_loss: 0.4349 - val_accuracy: 0.7988\n",
            "Epoch 28/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.8320\n",
            "Epoch 28: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 26s 103ms/step - loss: 0.3317 - accuracy: 0.8320 - val_loss: 0.4884 - val_accuracy: 0.8047\n",
            "Epoch 29/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2874 - accuracy: 0.8384\n",
            "Epoch 29: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 26s 100ms/step - loss: 0.2874 - accuracy: 0.8384 - val_loss: 0.4963 - val_accuracy: 0.7861\n",
            "Epoch 30/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2819 - accuracy: 0.8413\n",
            "Epoch 30: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 27s 104ms/step - loss: 0.2819 - accuracy: 0.8413 - val_loss: 0.6938 - val_accuracy: 0.7256\n",
            "Epoch 31/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2812 - accuracy: 0.8486\n",
            "Epoch 31: val_accuracy did not improve from 0.81348\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2812 - accuracy: 0.8486 - val_loss: 0.4434 - val_accuracy: 0.8057\n",
            "Epoch 32/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2691 - accuracy: 0.8506\n",
            "Epoch 32: val_accuracy improved from 0.81348 to 0.82324, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 30s 116ms/step - loss: 0.2691 - accuracy: 0.8506 - val_loss: 0.4117 - val_accuracy: 0.8232\n",
            "Epoch 33/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8347\n",
            "Epoch 33: val_accuracy did not improve from 0.82324\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3141 - accuracy: 0.8347 - val_loss: 0.9853 - val_accuracy: 0.6807\n",
            "Epoch 34/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.8523\n",
            "Epoch 34: val_accuracy improved from 0.82324 to 0.82715, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2515 - accuracy: 0.8523 - val_loss: 0.4157 - val_accuracy: 0.8271\n",
            "Epoch 35/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2593 - accuracy: 0.8481\n",
            "Epoch 35: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2593 - accuracy: 0.8481 - val_loss: 0.4752 - val_accuracy: 0.7969\n",
            "Epoch 36/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8311\n",
            "Epoch 36: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 27s 107ms/step - loss: 0.3288 - accuracy: 0.8311 - val_loss: 0.4745 - val_accuracy: 0.8086\n",
            "Epoch 37/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8311\n",
            "Epoch 37: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.3052 - accuracy: 0.8311 - val_loss: 0.4888 - val_accuracy: 0.7861\n",
            "Epoch 38/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8430\n",
            "Epoch 38: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 26s 102ms/step - loss: 0.2738 - accuracy: 0.8430 - val_loss: 0.5580 - val_accuracy: 0.7402\n",
            "Epoch 39/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2893 - accuracy: 0.8584\n",
            "Epoch 39: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2893 - accuracy: 0.8584 - val_loss: 0.4834 - val_accuracy: 0.7998\n",
            "Epoch 40/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.8584\n",
            "Epoch 40: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.2508 - accuracy: 0.8584 - val_loss: 0.4430 - val_accuracy: 0.7930\n",
            "Epoch 41/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.8542\n",
            "Epoch 41: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.2578 - accuracy: 0.8542 - val_loss: 0.7074 - val_accuracy: 0.7461\n",
            "Epoch 42/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8528\n",
            "Epoch 42: val_accuracy did not improve from 0.82715\n",
            "256/256 [==============================] - 26s 102ms/step - loss: 0.2769 - accuracy: 0.8528 - val_loss: 0.4404 - val_accuracy: 0.8164\n",
            "Epoch 43/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.8582\n",
            "Epoch 43: val_accuracy improved from 0.82715 to 0.84277, saving model to shuffled_model4_1-3.h5\n",
            "256/256 [==============================] - 30s 116ms/step - loss: 0.2486 - accuracy: 0.8582 - val_loss: 0.3834 - val_accuracy: 0.8428\n",
            "Epoch 44/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.8518\n",
            "Epoch 44: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.2669 - accuracy: 0.8518 - val_loss: 0.4571 - val_accuracy: 0.8096\n",
            "Epoch 45/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.8662\n",
            "Epoch 45: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 110ms/step - loss: 0.2437 - accuracy: 0.8662 - val_loss: 0.4303 - val_accuracy: 0.8145\n",
            "Epoch 46/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.8579\n",
            "Epoch 46: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 101ms/step - loss: 0.2727 - accuracy: 0.8579 - val_loss: 0.4680 - val_accuracy: 0.8125\n",
            "Epoch 47/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.8694\n",
            "Epoch 47: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 108ms/step - loss: 0.2384 - accuracy: 0.8694 - val_loss: 0.5361 - val_accuracy: 0.7803\n",
            "Epoch 48/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2370 - accuracy: 0.8672\n",
            "Epoch 48: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 107ms/step - loss: 0.2370 - accuracy: 0.8672 - val_loss: 0.5276 - val_accuracy: 0.7686\n",
            "Epoch 49/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.8599\n",
            "Epoch 49: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 29s 112ms/step - loss: 0.2477 - accuracy: 0.8599 - val_loss: 0.5121 - val_accuracy: 0.8096\n",
            "Epoch 50/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.8674\n",
            "Epoch 50: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 103ms/step - loss: 0.2241 - accuracy: 0.8674 - val_loss: 0.4549 - val_accuracy: 0.8174\n",
            "Epoch 51/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2547 - accuracy: 0.8542\n",
            "Epoch 51: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2547 - accuracy: 0.8542 - val_loss: 0.7769 - val_accuracy: 0.7070\n",
            "Epoch 52/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.8586\n",
            "Epoch 52: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 109ms/step - loss: 0.2609 - accuracy: 0.8586 - val_loss: 0.5661 - val_accuracy: 0.7734\n",
            "Epoch 53/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2416 - accuracy: 0.8711\n",
            "Epoch 53: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 100ms/step - loss: 0.2416 - accuracy: 0.8711 - val_loss: 0.4189 - val_accuracy: 0.8330\n",
            "Epoch 54/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2359 - accuracy: 0.8672\n",
            "Epoch 54: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 103ms/step - loss: 0.2359 - accuracy: 0.8672 - val_loss: 0.4818 - val_accuracy: 0.7998\n",
            "Epoch 55/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2600 - accuracy: 0.8628\n",
            "Epoch 55: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 109ms/step - loss: 0.2600 - accuracy: 0.8628 - val_loss: 0.7144 - val_accuracy: 0.7480\n",
            "Epoch 56/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.8684\n",
            "Epoch 56: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 27s 106ms/step - loss: 0.2388 - accuracy: 0.8684 - val_loss: 0.4785 - val_accuracy: 0.8027\n",
            "Epoch 57/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.8582\n",
            "Epoch 57: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 28s 109ms/step - loss: 0.2622 - accuracy: 0.8582 - val_loss: 0.4926 - val_accuracy: 0.8105\n",
            "Epoch 58/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2397 - accuracy: 0.8691\n",
            "Epoch 58: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 101ms/step - loss: 0.2397 - accuracy: 0.8691 - val_loss: 0.4293 - val_accuracy: 0.8174\n",
            "Epoch 59/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8538\n",
            "Epoch 59: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 26s 102ms/step - loss: 0.2554 - accuracy: 0.8538 - val_loss: 0.4001 - val_accuracy: 0.8252\n",
            "Epoch 60/60\n",
            "256/256 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8538\n",
            "Epoch 60: val_accuracy did not improve from 0.84277\n",
            "256/256 [==============================] - 27s 105ms/step - loss: 0.2865 - accuracy: 0.8538 - val_loss: 0.4704 - val_accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.evaluate(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe7b4kiNt24a",
        "outputId": "a23455e5-1551-44c6-cf1b-b227bf6878b7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 6s 75ms/step - loss: 0.4604 - accuracy: 0.8070\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.46044039726257324, 0.8070312738418579]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4.evaluate(train_images) , model4.evaluate(validation_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLAWM-L2xYg0",
        "outputId": "bdbc42c2-fd5c-440e-f51e-4f7569e631d6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256/256 [==============================] - 15s 57ms/step - loss: 0.2370 - accuracy: 0.8970\n",
            "64/64 [==============================] - 3s 49ms/step - loss: 0.3977 - accuracy: 0.8340\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.23701004683971405, 0.89697265625], [0.3976980447769165, 0.833984375])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkMExrUznR9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_test_labels = test_images.classes\n",
        "test_pred = model4.predict(test_images)\n",
        "\n",
        "test_pred_classes = np.argmax(test_pred, axis=1)\n",
        "confusion_Matrix = confusion_matrix(true_test_labels, test_pred_classes)\n",
        "sns.heatmap(confusion_Matrix, annot=True, cmap='Blues',fmt='.3g')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "kAevJ62rUG90",
        "outputId": "1a3eb376-80d2-40db-b982-6e8db06f1fd1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 4s 46ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_report = classification_report(true_test_labels, test_pred_classes)\n",
        "print(test_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1fiLLf2UbTq",
        "outputId": "49f035a1-4183-4754-cfbe-732c4aa20458"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.13      0.16      0.15       201\n",
            "           1       0.00      0.00      0.00         6\n",
            "           2       0.49      0.45      0.47       643\n",
            "           3       0.33      0.32      0.32       430\n",
            "\n",
            "    accuracy                           0.36      1280\n",
            "   macro avg       0.24      0.23      0.23      1280\n",
            "weighted avg       0.37      0.36      0.37      1280\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQvAf28uUm19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}